#!/usr/bin/env perl

## This script allows to download FASTQ files from SRA

use strict;
use warnings;
use diagnostics;
use Getopt::Long;
use Try::Tiny;
use FindBin;
use Cwd;
use lib "$FindBin::Bin/../../.."; # Get lib path for Utils.pm
use Utils;
use Time::Piece;
use File::Path qw(make_path remove_tree);
use File::Basename;
use Cpanel::JSON::XS;
use LWP::Simple;

## Define arguments & their default value
my ($metadataFile, $parallelJobs, $outputDir, $bamtofastq, $queue, $account, $downloadedLibraries) = ('', '', '', '', '',  '', '');
my $renameScriptPath = '';
my $doNotDownload = (0);
my %opts = ('metadataFile=s'        => \$metadataFile,
            'parallelJobs=s'        => \$parallelJobs,
            'downloadedLibraries=s' => \$downloadedLibraries,
            'outputDir=s'           => \$outputDir,
            'bamtofastq=s'          => \$bamtofastq,
            'queue=s'               => \$queue,
            'account=s'             => \$account,
            'doNotDownload'         => \$doNotDownload,
            'renameScriptPath=s'    => \$renameScriptPath
           );


######################## Check arguments ########################
my $test_options = Getopt::Long::GetOptions(%opts);
if ( !$metadataFile || $parallelJobs eq '' || $outputDir eq '' || $bamtofastq eq '' ||
    $queue eq '' || $account eq '' || $renameScriptPath eq '') {
    print "\n\tInvalid or missing argument:
\t-metadataFile            file containing metadata necessary to download each run
\t-parallelJobs            maximum number of jobs to run in parallel
\t-downloadedLibraries     file containing the ID of all alreaydy downloaded libraries for single cell
\t-outputDir               directory where FASTQ files are downloaded/generated
\t-bamtofastq              directory where the bamtofastq tool from 10X is installed
\t-queue                   queue to use to run jobs on the cluster
\t-account                 account to use to run jobs on the cluster
\t-renameScriptPath        path to the script allowing to rename fastq files generated by the bamtofastq tool
\t-doNotDownload           (optional) option used to check libraries that have to be downloaded without downloading them.It generates symlink for libraries
                           properly downloaded and add them to the file listing already downloaded libraries. This option is useful if the script has been killed
                           before generating symlink or updating the file listing download libraries. This can happen when the download is too long. The script can
                           then be killed by cluster admins.
\n";
    exit 1;
}

require("$FindBin::Bin/../../rna_seq_utils.pl");
require("$FindBin::Bin/../../target_base_utils.pl");


#### Functions ####

# Extract path to fastq files for fastq files on ArrayExpress

sub get_arrayExpress_fastq_files {
    my ($arrayExpressId, $runId) = @_;
    my $arrayExpressUrlPrefix = "https://ftp.ebi.ac.uk/biostudies/nfs/";
    $arrayExpressId =~ /^(\w{1}-\w{4}-)\d+(\d{3})$/ || die "worng format for arrayexpress ID\n";
    my ($arrayExpressIdPrefix, $arrayExpressIdSuffix) = ($1, $2);
    my $arrayExpressUrl = "$arrayExpressUrlPrefix/$arrayExpressIdPrefix/$arrayExpressIdSuffix/$arrayExpressId/Files/${arrayExpressId}.sdrf.txt";
    my $text = get $arrayExpressUrl;
    open( FILE_HANDLER, '<', \$text ) or die "failed to read input file: $!";
    my @fastqFiles = ();
    my $runIdIndex = "";
    my @fastqIndexes = ();
    while (my $line = <FILE_HANDLER>) {
        my @tmp = map { bgeeTrim($_) } map { s/^\"//; s/\"$//; $_ } split(/\t/, $line, -1);

        #number of columns varies so position of columns containing runId and fastq files is not static.
        #we first parse the header to find the position of the columns of interest
        if ( $line =~ /^Source Name/ ) {
            my $numberColumns = scalar @tmp;
            foreach my $index (0..$numberColumns) {
                next if !$tmp[$index];
                if ($tmp[$index] eq "Comment[ENA_RUN]") {
                    $runIdIndex = $index;
                } elsif ($tmp[$index] eq "Comment[FASTQ_URI]") {
                    push @fastqIndexes, $index;
                }
            }
        }
        my $numberFastqFiles = scalar @fastqIndexes;
        die "uncorrect number of fastq files for run $runId. Expected 3 but found $numberFastqFiles\n"  if ($numberFastqFiles != 3 && $numberFastqFiles != 2);
        my $arrayExpressRunId = $tmp[$runIdIndex];
        next if $arrayExpressRunId ne $runId;
        for my $index (@fastqIndexes) {
            push @fastqFiles, $tmp[$index];
        }
        last;
    }
    close FILE_HANDLER;
    return @fastqFiles;
}

##### Main #####

# Info of processed libraries coming from the pipeline
my $isTargetBased = 1;
my %processedLibraries = get_processed_libraries_info($metadataFile, $isTargetBased);

my $experimentDirName = "EXPERIMENTS";
my $experimentOutputDir = "$outputDir/$experimentDirName";

# Read already downloaded libraries
my %alreadyDownloaded = map { $_ => 1 } read_file("$downloadedLibraries", chomp=>1);

my %sbatchToRun = ();

## create directory
my $jobPrefix = "download_";
my $sbatchDir = "$outputDir/${jobPrefix}sbatch";
my $clusterOutputDir = "$outputDir/${jobPrefix}clusterOutput";
make_path("$sbatchDir");
make_path("$clusterOutputDir");

#store initial dir location to be able to move for symlink generation and then come back later
my $initialDir = getcwd;

my $jobs_created = 0;
## first create sbatch files and add them to an array of sbatch to run
foreach my $experimentId (keys %processedLibraries){
    foreach my $libraryId (keys %{$processedLibraries{$experimentId}}){
        my $speciesId = $processedLibraries{$experimentId}{$libraryId}{'speciesId'};
        my $libDirectory = "$outputDir/$speciesId/$libraryId";
        next if ( exists $alreadyDownloaded{$libraryId} );
        foreach my $runId (keys %{$processedLibraries{$experimentId}{$libraryId}}){
            next if $runId eq "speciesId";
            my $runDirectory = "$libDirectory/$runId";
            if(! $doNotDownload) {
                next if (-f "$runDirectory/done");
            }
            #create sbatch file and
            my $source = $processedLibraries{$experimentId}{$libraryId}{$runId}{'downloadSource'};
            my $jobName = "$jobPrefix$runId";
            ## Use 4Gb of memory. Should maybe be increase depending on the run to download
            ## ask for 4 cpus as it is the number of threads used by the bamtofastq tool
            my $sbatchTemplate = Utils::sbatch_template($queue, $account, 4,
              50, "$clusterOutputDir/$jobName.out", "$clusterOutputDir/$jobName.err",
              $jobName);
            $sbatchTemplate .= "export bamtofastq=$bamtofastq\n";
            my $submittedFtp = $processedLibraries{$experimentId}{$libraryId}{$runId}{'submittedFTP'};
            my @bamInfos = split(";", $submittedFtp);
            $sbatchTemplate .= "module load gcc/10.4.0;\nmodule load fastp/0.23.2;\nexport PATH=/software/bin:\$PATH;\n";
            if ($source eq "SRA" && $experimentId =~ m/^SRP/) {
                ## load SRA sra-toolkit
                $sbatchTemplate .= "module load sratoolkit/3.0.0;\n";
                ## download fastq from SRA
                if ($submittedFtp eq 'NA') {
                    $sbatchTemplate .= "fastq-dump --outdir $runDirectory/FASTQ --split-files $runId &&\n";
                    $sbatchTemplate .= "perl $renameScriptPath -fastqPath=$runDirectory/FASTQ/ -runId=$runId -renaming=fastqdump &&\n";
                    $sbatchTemplate .= "find $runDirectory/FASTQ/ \\( -name '*.fastq' -o -name '*.fq' \\) -exec gzip {} \\; &&\n";
                ## download BAM from SRA
                } else {
                    # download bam file
                    $sbatchTemplate .= "{\nprefetch --type bam --max-size 9999999999 -O $runDirectory $runId ||\n";
                    $sbatchTemplate .= "wget --no-verbose --directory-prefix=$runDirectory $bamInfos[0]\n} &&\n";
                    # remove FASTQ dir if it already exists
                    $sbatchTemplate .= "if [ -d $runDirectory/FASTQ ]; then rm -rf $runDirectory/FASTQ; fi &&\n";
                    # transform bam file to fastq file
                    $sbatchTemplate .= "$bamtofastq --reads-per-fastq=90000000000 --nthreads=4 $runDirectory/".basename($bamInfos[0])." $runDirectory/FASTQ &&\n";
                    #check that bam to fastq created the output directory. As it does not quit with an error when the bam file is corrupted, this
                    # check allows to test if bamtofastq succeeded in fastq files creation
                    $sbatchTemplate .= "find $runDirectory/FASTQ/ >/dev/null 2>&1\n";
                    $sbatchTemplate .= "if [ \$? -ne 0 ];then echo \"bamtofastq failed to generate fastq files\" >&2; exit 1; fi &&\n";
                    # move files to the proper directory
                    $sbatchTemplate .= "find $runDirectory/FASTQ/ \\( -name '*.fastq.gz' -o -name '*.fq.gz' \\) -exec mv {} $runDirectory/FASTQ \\; >/dev/null 2>&1\n";
                    # remove directories in the FASTQ directy
                    $sbatchTemplate .= "find $runDirectory/FASTQ/ -mindepth 1 -maxdepth 1 -type d -exec rm -rf {} \\; &&\n";
                    #rename and merge fastq files generated by bamtofastq
                    $sbatchTemplate .= "perl $renameScriptPath -fastqPath=$runDirectory/FASTQ/ -runId=$runId -renaming=bamtofastq &&\n";
                    #remove bam file
                    $sbatchTemplate .= "rm $runDirectory/".basename($bamInfos[0])." &&\n";
                }
            } elsif ($source eq "EBI" || $source eq "HCA" || ($source eq "SRA" && $experimentId =~ m/^ERP/)) {
                # if have to download fastq files
                if ($bamInfos[0] =~ m/\.fastq\.gz$/) {
                    foreach my $bamInfo (@bamInfos) {
                        #download
                        $sbatchTemplate .= "wget --no-verbose --directory-prefix=$runDirectory $bamInfo &&\n";
                        if ($bamInfo =~ m/_I1_/ || $bamInfo =~ m/_I1.f/) {
                            $sbatchTemplate .= "mv $runDirectory/".basename($bamInfo)." $runDirectory/FASTQ/".$runId."_I1.fastq.gz &&\n";
                        } elsif ($bamInfo =~ m/_R1_/ || $bamInfo =~ m/_R1.f/) {
                            $sbatchTemplate .= "mv $runDirectory/".basename($bamInfo)." $runDirectory/FASTQ/".$runId."_R1.fastq.gz &&\n";
                        } elsif ($bamInfo =~ m/_R2_/ || $bamInfo =~ m/_R2.f/) {
                            $sbatchTemplate .= "mv $runDirectory/".basename($bamInfo)." $runDirectory/FASTQ/".$runId."_R2.fastq.gz &&\n";
                        } else {
                            warn "did not manage to rename the file $bamInfo";
                        }
                    }
                } elsif ($bamInfos[0] =~ m/\.bam$/) {
                    # this approach solve issues of bam files not transformable to R1 and R2 fastq files (either 10X bamtofastq does not work
                    # or fastq-dump download one single fastq file named RUN_ID_1.fastq)
                    my $jsonContent = get("https://www.ebi.ac.uk/biostudies/api/v1/arrayexpress/search?query=$experimentId");
                    my $json = decode_json($jsonContent);
                    my $arrayExpressId = $json->{'hits'}->[0]->{'accession'};
                    my @fastqFiles = get_arrayExpress_fastq_files($arrayExpressId, $runId);
                    foreach my $fastqFile (@fastqFiles) {
                        #download
                        $sbatchTemplate .= "wget --directory-prefix=$runDirectory $fastqFile &&\n";
                        if ($fastqFile =~ m/_I1_/ || $fastqFile =~ m/_I1.f/ || $fastqFile =~ m/.I1./) {
                            $sbatchTemplate .= "mv $runDirectory/".basename($fastqFile)." $runDirectory/FASTQ/".$runId."_I1.fastq.gz &&\n";
                        } elsif ($fastqFile =~ m/_R1_/ || $fastqFile =~ m/_R1.f/ || $fastqFile =~ m/.R1./) {
                            $sbatchTemplate .= "mv $runDirectory/".basename($fastqFile)." $runDirectory/FASTQ/".$runId."_R1.fastq.gz &&\n";
                        } elsif ($fastqFile =~ m/_R2_/ || $fastqFile =~ m/_R2.f/ || $fastqFile =~ m/.R2.f/) {
                            $sbatchTemplate .= "mv $runDirectory/".basename($fastqFile)." $runDirectory/FASTQ/".$runId."_R2.fastq.gz &&\n";
                        } else {
                            warn "did not manage to rename the file $fastqFile for run $runId\n";
                        }
                    }
                } else {
                    warn "unrecognized file extension for $bamInfos[0]";
                }
            } else {
                warn "unrecognized source $source for run $runId\n";
                next;
            }
	    $sbatchTemplate .= "perl 0Preparation/run_fastp.pl -run_path=$runDirectory -run_id=$runId && \n";
            $jobs_created++;
            $sbatchTemplate .= "touch $runDirectory/done";
            ## create sbatch file and add its path to the hash of sbatch files
            my $sbatchFilePath = "$sbatchDir/$jobName.sbatch";
            $sbatchToRun{$experimentId}{$libraryId}{$runId} = $sbatchFilePath;
            $sbatchToRun{$experimentId}{$libraryId}{'speciesId'} = $speciesId;
            open(FH, '>', $sbatchFilePath) or die $!;
            print FH $sbatchTemplate;
            close(FH);
        }
    }
}

print "created $jobs_created sbatch files.\n";

# if jobs had to be run
if ($jobs_created > 0) {
    
    my $numberJobRun = 0;
    my $startTime = localtime->strftime('%Y-%m-%dT%H:%M:%S');

    if(! $doNotDownload) {
        my $jobsRunning = Utils::check_active_jobs_number_per_account_and_name($account, $jobPrefix);
        foreach my $experimentId (keys %sbatchToRun){
            foreach my $libraryId (keys %{$sbatchToRun{$experimentId}}){
                my $speciesId = $processedLibraries{$experimentId}{$libraryId}{'speciesId'};
                my $libDirectory = "$outputDir/$speciesId/$libraryId";
                foreach my $runId (keys %{$sbatchToRun{$experimentId}{$libraryId}}){
                    next if $runId eq "speciesId";
                    my $runDirectory = "$libDirectory/$runId";
                    next if (-f "$runDirectory/done");
                    $numberJobRun++;
                    while ($jobsRunning >= $parallelJobs) {
                            sleep(15);
                            $jobsRunning = Utils::check_active_jobs_number_per_account_and_name($account, $jobPrefix);
                        }
                    make_path("$runDirectory/FASTQ");
                    chdir "$libDirectory";
                    system("sbatch $sbatchToRun{$experimentId}{$libraryId}{$runId}");
                    $jobsRunning = Utils::check_active_jobs_number_per_account_and_name($account, $jobPrefix);

                }
            }
        }

        while ($jobsRunning > 0) {
            sleep(15);
            $jobsRunning = Utils::check_active_jobs_number_per_account_and_name($account, $jobPrefix);
        }
    }

    print "all download jobs finished. Run $numberJobRun jobs.\n";

    my %jobs_status = Utils::count_status_finished_jobs($jobPrefix, $startTime);
    print $jobs_status{"completed"}." jobs completed, ".$jobs_status{"failed"}." jobs failed, ".$jobs_status{"out_of_memory"}." jobs failed with an out of memory issue and ".$jobs_status{"cancelled"}." jobs have been cancelled.\n";

    # wait 5 seconds to be sure all .done files had enough time to be created. Done to solve issues when cluster
    # is reeeeaaaaally slow
    sleep(5);
    print "now start to generate symlinks of libraries per species\n";
    foreach my $experimentId (keys %sbatchToRun){
        foreach my $libraryId (keys %{$sbatchToRun{$experimentId}}){
            my $speciesId = $sbatchToRun{$experimentId}{$libraryId}{'speciesId'};
            my $libDirectory = "$outputDir/$speciesId/$libraryId";
            my $done = 1;
            foreach my $runId (keys %{$sbatchToRun{$experimentId}{$libraryId}}){
                next if $runId eq "speciesId";
                if(! -e "$libDirectory/$runId/done") {
                    $done = 0;
                }
            }
            if ($done) {
                # if all the run of this library have been properly downloaded then
                # we generate a symlink
                print "done download of library $libraryId\n";
                my $currentExpDir = "$experimentOutputDir/$experimentId";
                make_path("$currentExpDir");
                chdir "$currentExpDir";
                system("ln -s ../../$speciesId/$libraryId $libraryId");
                # we also add this library to the list of already generated libraries
                $alreadyDownloaded{$libraryId} = 1;
            } else {
                warn "Did not properly download the library $libraryId";
                remove_tree("$libDirectory");
            }
        }
    }

    #now go back to original location to update file listing all downloaded libraries
    chdir "$initialDir";
    print "Finally update the file containing downloaded libraries";
    open my $outFh, "> ", "$downloadedLibraries" or die "Cannot write: $! \n";
    foreach my $libraryId (sort keys %alreadyDownloaded) {
        print $outFh "$libraryId\n";
    }
    close $outFh;
}
